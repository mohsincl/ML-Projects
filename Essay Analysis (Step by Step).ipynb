{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing required libraries: Regex operations, pandas, numpy, SVC model\n",
    "import re                  \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_essay_training(data, essay_set, essay, score):\n",
    "    if essay_set not in data:\n",
    "        data[essay_set] = {\"essay\":[],\"score\":[]}\n",
    "    data[essay_set][\"essay\"].append(essay)\n",
    "    data[essay_set][\"score\"].append(score)\n",
    "\n",
    "def add_essay_test(data, essay_set, essay, prediction_id):\n",
    "    if essay_set not in data:\n",
    "        data[essay_set] = {\"essay\":[], \"prediction_id\":[]}\n",
    "    data[essay_set][\"essay\"].append(essay)\n",
    "    data[essay_set][\"prediction_id\"].append(prediction_id)\n",
    "\n",
    "def read_training_data(training_file):\n",
    "    f = open(training_file)\n",
    "    f.readline()\n",
    "\n",
    "    training_data = {}\n",
    "    for row in f:\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        essay_set = row[1]\n",
    "        essay = row[2]\n",
    "        domain1_score = int(row[6])\n",
    "        if essay_set == \"2\":\n",
    "            essay_set = \"2_1\"\n",
    "        add_essay_training(training_data, essay_set, essay, domain1_score)\n",
    "        \n",
    "        if essay_set == \"2_1\":\n",
    "            essay_set = \"2_2\"\n",
    "            domain2_score = int(row[9])\n",
    "            add_essay_training(training_data, essay_set, essay, domain2_score)\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "def read_test_data(test_file):\n",
    "    f = open(test_file)\n",
    "    f.readline()\n",
    "\n",
    "    test_data = {}\n",
    "    for row in f:\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        essay_set = row[1]\n",
    "        essay = row[2]\n",
    "        domain1_predictionid = int(row[3])\n",
    "        if essay_set == \"2\": \n",
    "            domain2_predictionid = int(row[4])\n",
    "            add_essay_test(test_data, \"2_1\", essay, domain1_predictionid)\n",
    "            add_essay_test(test_data, \"2_2\", essay, domain2_predictionid)\n",
    "        else:\n",
    "            add_essay_test(test_data, essay_set, essay, domain1_predictionid)\n",
    "    return test_data\n",
    "\n",
    "def get_character_count(essay):\n",
    "    return len(essay)\n",
    "\n",
    "def get_word_count(essay):\n",
    "    return len(re.findall(r\"\\s\", essay))+1\n",
    "\n",
    "def extract_features(essays, feature_functions):\n",
    "    return [[f(es) for f in feature_functions] for es in essays]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "Reading Validation Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Training Data\")\n",
    "training = read_training_data(\"Desktop/hackathon/ASAP-AES/Data/training_set_rel3.tsv\")\n",
    "print(\"Reading Validation Data\")\n",
    "test = read_test_data(\"Desktop/hackathon/ASAP-AES/Data/valid_set.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>rater1_trait1</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          essay_set                                              essay  \\\n",
       "essay_id                                                                 \n",
       "1                 1  Dear local newspaper, I think effects computer...   \n",
       "2                 1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "3                 1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "4                 1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "5                 1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "          rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "essay_id                                                                  \n",
       "1                      4               4             NaN              8   \n",
       "2                      5               4             NaN              9   \n",
       "3                      4               3             NaN              7   \n",
       "4                      5               5             NaN             10   \n",
       "5                      4               4             NaN              8   \n",
       "\n",
       "          rater1_domain2  rater2_domain2  domain2_score  rater1_trait1  \\\n",
       "essay_id                                                                 \n",
       "1                    NaN             NaN            NaN            NaN   \n",
       "2                    NaN             NaN            NaN            NaN   \n",
       "3                    NaN             NaN            NaN            NaN   \n",
       "4                    NaN             NaN            NaN            NaN   \n",
       "5                    NaN             NaN            NaN            NaN   \n",
       "\n",
       "              ...        rater2_trait3  rater2_trait4  rater2_trait5  \\\n",
       "essay_id      ...                                                      \n",
       "1             ...                  NaN            NaN            NaN   \n",
       "2             ...                  NaN            NaN            NaN   \n",
       "3             ...                  NaN            NaN            NaN   \n",
       "4             ...                  NaN            NaN            NaN   \n",
       "5             ...                  NaN            NaN            NaN   \n",
       "\n",
       "          rater2_trait6  rater3_trait1  rater3_trait2  rater3_trait3  \\\n",
       "essay_id                                                               \n",
       "1                   NaN            NaN            NaN            NaN   \n",
       "2                   NaN            NaN            NaN            NaN   \n",
       "3                   NaN            NaN            NaN            NaN   \n",
       "4                   NaN            NaN            NaN            NaN   \n",
       "5                   NaN            NaN            NaN            NaN   \n",
       "\n",
       "          rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "essay_id                                               \n",
       "1                   NaN            NaN            NaN  \n",
       "2                   NaN            NaN            NaN  \n",
       "3                   NaN            NaN            NaN  \n",
       "4                   NaN            NaN            NaN  \n",
       "5                   NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strain = DataFrame.from_csv(\"Desktop/hackathon/ASAP-AES/Data/training_set_rel3.tsv\", sep=\"\\t\")\n",
    "strain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>rater1_trait1</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          essay_set                                              essay  \\\n",
       "essay_id                                                                 \n",
       "1                 1  Dear local newspaper, I think effects computer...   \n",
       "2                 1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "3                 1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "4                 1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "5                 1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "          rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "essay_id                                                                  \n",
       "1                      4               4             NaN              8   \n",
       "2                      5               4             NaN              9   \n",
       "3                      4               3             NaN              7   \n",
       "4                      5               5             NaN             10   \n",
       "5                      4               4             NaN              8   \n",
       "\n",
       "          rater1_domain2  rater2_domain2  domain2_score  rater1_trait1  \\\n",
       "essay_id                                                                 \n",
       "1                    NaN             NaN            NaN            NaN   \n",
       "2                    NaN             NaN            NaN            NaN   \n",
       "3                    NaN             NaN            NaN            NaN   \n",
       "4                    NaN             NaN            NaN            NaN   \n",
       "5                    NaN             NaN            NaN            NaN   \n",
       "\n",
       "              ...        rater2_trait3  rater2_trait4  rater2_trait5  \\\n",
       "essay_id      ...                                                      \n",
       "1             ...                  NaN            NaN            NaN   \n",
       "2             ...                  NaN            NaN            NaN   \n",
       "3             ...                  NaN            NaN            NaN   \n",
       "4             ...                  NaN            NaN            NaN   \n",
       "5             ...                  NaN            NaN            NaN   \n",
       "\n",
       "          rater2_trait6  rater3_trait1  rater3_trait2  rater3_trait3  \\\n",
       "essay_id                                                               \n",
       "1                   NaN            NaN            NaN            NaN   \n",
       "2                   NaN            NaN            NaN            NaN   \n",
       "3                   NaN            NaN            NaN            NaN   \n",
       "4                   NaN            NaN            NaN            NaN   \n",
       "5                   NaN            NaN            NaN            NaN   \n",
       "\n",
       "          rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "essay_id                                               \n",
       "1                   NaN            NaN            NaN  \n",
       "2                   NaN            NaN            NaN  \n",
       "3                   NaN            NaN            NaN  \n",
       "4                   NaN            NaN            NaN  \n",
       "5                   NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stest = DataFrame.from_csv(\"Desktop/hackathon/ASAP-AES/Data/training_set_rel3.tsv\", sep=\"\\t\")\n",
    "stest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_functions = [get_character_count, get_word_count]\n",
    "\n",
    "essay_sets = sorted(training.keys())\n",
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Predictions for Essay Set 1\n",
      "Making Predictions for Essay Set 2_1\n",
      "Making Predictions for Essay Set 2_2\n",
      "Making Predictions for Essay Set 3\n",
      "Making Predictions for Essay Set 4\n",
      "Making Predictions for Essay Set 5\n",
      "Making Predictions for Essay Set 6\n",
      "Making Predictions for Essay Set 7\n",
      "Making Predictions for Essay Set 8\n",
      "[[4018, 738], [4671, 867], [3403, 658], [4707, 844], [4355, 860], [3469, 675], [1433, 262], [1960, 389], [2035, 379], [3382, 629], [2167, 404], [3831, 732], [2333, 465], [4562, 849], [2643, 530], [2256, 430], [3465, 627], [4067, 761], [2753, 529], [1664, 318], [1677, 333], [4587, 852], [3813, 719], [4341, 823], [4236, 853], [2702, 499], [3003, 565], [1515, 287], [1828, 344], [1785, 345], [3249, 609], [3217, 649], [4433, 845], [4535, 848], [3463, 727], [2115, 388], [3427, 627], [2705, 585], [4423, 833], [2855, 543], [2209, 420], [4714, 847], [2308, 501], [3409, 704], [3101, 614], [4161, 838], [3061, 627], [4151, 767], [4163, 766], [3419, 677], [4147, 850], [2085, 389], [3575, 689], [4074, 805], [2536, 521], [3024, 551], [4719, 874], [4294, 869], [2828, 518], [4384, 856], [3324, 600], [3806, 710], [4705, 851], [4798, 857], [1455, 282], [4447, 853], [4217, 779], [3731, 709], [4520, 850], [4198, 853], [4430, 815], [2335, 469], [2888, 545], [2629, 502], [2859, 544], [2609, 454], [3725, 731], [3437, 640], [3858, 683], [3379, 668], [2130, 444], [4729, 902], [4379, 814], [4081, 806], [2395, 451], [3365, 696], [289, 57], [3604, 759], [4566, 854], [4002, 840], [2507, 491], [4686, 845], [3204, 628], [2083, 414], [4499, 876], [2818, 561], [3351, 655], [3385, 625], [3491, 622], [4954, 852], [4298, 853], [2384, 444], [3677, 717], [2306, 317], [4304, 850], [3639, 676], [4344, 852], [3820, 720], [4428, 854], [4236, 847], [4220, 797], [3440, 648], [3504, 671], [4386, 843], [4314, 815], [3720, 740], [3842, 708], [3687, 768], [4586, 835], [4447, 882], [2889, 574], [4579, 848], [1959, 416], [3938, 804], [3745, 738], [2119, 401], [2064, 416], [1098, 207], [3155, 619], [2883, 585], [4464, 870], [3012, 603], [3112, 605], [3186, 609], [2469, 471], [2355, 519], [2803, 501], [2944, 584], [2521, 506], [2986, 622], [2004, 387], [2432, 416], [3837, 787], [4570, 842], [1129, 264], [2255, 445], [2529, 490], [4486, 849], [4435, 855], [3004, 598], [3161, 584], [2524, 459], [2788, 507], [4462, 824], [2380, 433], [2104, 407], [3127, 605], [4100, 774], [2522, 529], [4203, 846], [2031, 384], [3377, 715], [3063, 590], [1713, 373], [3259, 631], [2351, 438], [2538, 515], [4154, 869], [3272, 593], [948, 177], [4749, 852], [1565, 299], [2161, 397], [1807, 367], [4698, 849], [2744, 512], [3007, 597], [2758, 544], [2124, 375], [2932, 566], [4864, 853], [4930, 839], [4205, 851], [4556, 857], [2221, 412], [4173, 856], [2831, 574], [4421, 850], [1381, 272], [3463, 635], [4708, 876], [4432, 854], [3940, 762], [4530, 860], [4065, 796], [2750, 505], [4913, 859], [3050, 603], [1034, 214], [1002, 199], [3600, 687], [2887, 621], [1389, 305], [4037, 751], [4590, 855], [1654, 300], [4398, 809], [4420, 849], [4072, 806], [4254, 759], [788, 159], [1962, 377], [4415, 841], [1343, 256], [4226, 855], [497, 112], [4324, 850], [4155, 800], [3186, 612], [4358, 843], [4664, 854], [4397, 841], [2835, 593], [2427, 481], [2763, 550], [3062, 606], [3655, 732], [3647, 772], [2892, 572], [3650, 743], [2952, 552], [2852, 569], [3175, 600]]\n"
     ]
    }
   ],
   "source": [
    "for es_set in essay_sets:\n",
    "        print(\"Making Predictions for Essay Set %s\" % es_set)\n",
    "        features = extract_features(training[es_set][\"essay\"],feature_functions)\n",
    "        rf = RandomForestRegressor(n_estimators = 100)\n",
    "        rf.fit(features,training[es_set][\"score\"])\n",
    "        features = extract_features(test[es_set][\"essay\"], feature_functions)\n",
    "        predicted_scores = rf.predict(features)\n",
    "        for pred_id, pred_score in zip(test[es_set][\"prediction_id\"], \n",
    "                                       predicted_scores):\n",
    "            predictions[pred_id] = round(pred_score)\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 40.93  39.47  38.99  37.98  41.98  37.75  34.39  36.39  36.21  40.3   37.2\n",
      "  42.54  34.42  38.67  37.94  32.6   41.74  38.11  38.09  32.41  33.3\n",
      "  40.31  40.76  39.71  36.15  37.02  38.92  33.56  37.83  34.67  42.1\n",
      "  37.51  40.82  36.89  35.58  38.16  38.18  37.04  38.44  36.59  39.22\n",
      "  39.26  32.19  30.68  35.46  33.79  36.78  36.58  37.16  37.51  33.58\n",
      "  37.79  38.21  36.39  37.79  39.31  41.68  42.1   38.75  38.11  40.9\n",
      "  39.54  38.85  42.78  34.72  39.65  43.8   38.04  40.09  35.39  41.49\n",
      "  33.21  33.06  35.94  37.3   41.06  40.2   40.38  43.98  37.99  36.2\n",
      "  41.77  40.63  36.49  37.08  35.08  20.82  32.44  39.8   32.35  33.93\n",
      "  37.11  39.02  30.85  43.8   33.57  35.02  38.99  42.58  42.89  37.89\n",
      "  37.39  37.67  38.26  33.28  38.79  42.62  40.71  39.9   37.79  41.71\n",
      "  41.09  41.99  39.61  40.53  35.46  41.99  34.76  37.08  41.79  33.57\n",
      "  40.01  30.2   34.91  36.03  36.43  31.09  30.1   36.49  32.97  44.73\n",
      "  38.55  36.57  38.24  37.9   34.21  37.24  35.9   34.57  35.13  35.68\n",
      "  38.81  35.14  39.95  26.51  32.67  35.26  39.01  39.78  38.98  42.5\n",
      "  39.47  37.62  43.88  38.01  39.31  35.41  38.68  37.36  33.85  37.56\n",
      "  34.25  36.82  31.71  40.68  37.57  35.37  36.    40.25  30.39  45.33\n",
      "  31.4   38.09  32.29  38.76  37.13  39.64  36.08  36.87  36.64  42.34\n",
      "  48.85  32.59  39.92  38.37  37.39  35.5   38.95  28.43  42.06  39.97\n",
      "  40.06  39.51  41.5   39.05  35.55  39.89  36.08  25.59  26.91  38.07\n",
      "  32.72  27.69  40.2   40.69  32.16  40.43  39.14  36.45  43.9   24.88\n",
      "  37.4   39.31  29.17  36.77  28.11  35.32  37.03  38.69  40.75  41.62\n",
      "  38.98  38.27  36.22  35.62  35.81  38.21  32.77  33.55  35.63  37.93\n",
      "  32.42  38.93]\n"
     ]
    }
   ],
   "source": [
    "print predicted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission to length_benchmark_rf.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = \"length_benchmark_rf.csv\"\n",
    "print(\"Writing submission to %s\" % output_file)\n",
    "f = open(output_file, \"w\")\n",
    "f.write(\"prediction_id,predicted_score\\n\")\n",
    "for key in sorted(predictions.keys()):\n",
    "   f.write(\"%d,%d\\n\" % (key,predictions[key]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Predictions for Essay Set 1\n",
      "Making Predictions for Essay Set 2_1\n",
      "Making Predictions for Essay Set 2_2\n",
      "Making Predictions for Essay Set 3\n",
      "Making Predictions for Essay Set 4\n",
      "Making Predictions for Essay Set 5\n",
      "Making Predictions for Essay Set 6\n",
      "Making Predictions for Essay Set 7\n",
      "Making Predictions for Essay Set 8\n",
      "[[4018, 738], [4671, 867], [3403, 658], [4707, 844], [4355, 860], [3469, 675], [1433, 262], [1960, 389], [2035, 379], [3382, 629], [2167, 404], [3831, 732], [2333, 465], [4562, 849], [2643, 530], [2256, 430], [3465, 627], [4067, 761], [2753, 529], [1664, 318], [1677, 333], [4587, 852], [3813, 719], [4341, 823], [4236, 853], [2702, 499], [3003, 565], [1515, 287], [1828, 344], [1785, 345], [3249, 609], [3217, 649], [4433, 845], [4535, 848], [3463, 727], [2115, 388], [3427, 627], [2705, 585], [4423, 833], [2855, 543], [2209, 420], [4714, 847], [2308, 501], [3409, 704], [3101, 614], [4161, 838], [3061, 627], [4151, 767], [4163, 766], [3419, 677], [4147, 850], [2085, 389], [3575, 689], [4074, 805], [2536, 521], [3024, 551], [4719, 874], [4294, 869], [2828, 518], [4384, 856], [3324, 600], [3806, 710], [4705, 851], [4798, 857], [1455, 282], [4447, 853], [4217, 779], [3731, 709], [4520, 850], [4198, 853], [4430, 815], [2335, 469], [2888, 545], [2629, 502], [2859, 544], [2609, 454], [3725, 731], [3437, 640], [3858, 683], [3379, 668], [2130, 444], [4729, 902], [4379, 814], [4081, 806], [2395, 451], [3365, 696], [289, 57], [3604, 759], [4566, 854], [4002, 840], [2507, 491], [4686, 845], [3204, 628], [2083, 414], [4499, 876], [2818, 561], [3351, 655], [3385, 625], [3491, 622], [4954, 852], [4298, 853], [2384, 444], [3677, 717], [2306, 317], [4304, 850], [3639, 676], [4344, 852], [3820, 720], [4428, 854], [4236, 847], [4220, 797], [3440, 648], [3504, 671], [4386, 843], [4314, 815], [3720, 740], [3842, 708], [3687, 768], [4586, 835], [4447, 882], [2889, 574], [4579, 848], [1959, 416], [3938, 804], [3745, 738], [2119, 401], [2064, 416], [1098, 207], [3155, 619], [2883, 585], [4464, 870], [3012, 603], [3112, 605], [3186, 609], [2469, 471], [2355, 519], [2803, 501], [2944, 584], [2521, 506], [2986, 622], [2004, 387], [2432, 416], [3837, 787], [4570, 842], [1129, 264], [2255, 445], [2529, 490], [4486, 849], [4435, 855], [3004, 598], [3161, 584], [2524, 459], [2788, 507], [4462, 824], [2380, 433], [2104, 407], [3127, 605], [4100, 774], [2522, 529], [4203, 846], [2031, 384], [3377, 715], [3063, 590], [1713, 373], [3259, 631], [2351, 438], [2538, 515], [4154, 869], [3272, 593], [948, 177], [4749, 852], [1565, 299], [2161, 397], [1807, 367], [4698, 849], [2744, 512], [3007, 597], [2758, 544], [2124, 375], [2932, 566], [4864, 853], [4930, 839], [4205, 851], [4556, 857], [2221, 412], [4173, 856], [2831, 574], [4421, 850], [1381, 272], [3463, 635], [4708, 876], [4432, 854], [3940, 762], [4530, 860], [4065, 796], [2750, 505], [4913, 859], [3050, 603], [1034, 214], [1002, 199], [3600, 687], [2887, 621], [1389, 305], [4037, 751], [4590, 855], [1654, 300], [4398, 809], [4420, 849], [4072, 806], [4254, 759], [788, 159], [1962, 377], [4415, 841], [1343, 256], [4226, 855], [497, 112], [4324, 850], [4155, 800], [3186, 612], [4358, 843], [4664, 854], [4397, 841], [2835, 593], [2427, 481], [2763, 550], [3062, 606], [3655, 732], [3647, 772], [2892, 572], [3650, 743], [2952, 552], [2852, 569], [3175, 600]]\n"
     ]
    }
   ],
   "source": [
    "for es_set in essay_sets:\n",
    "        print(\"Making Predictions for Essay Set %s\" % es_set)\n",
    "        features = extract_features(training[es_set][\"essay\"],feature_functions)\n",
    "        clf = SVC()\n",
    "        clf.fit(features,training[es_set][\"score\"]) \n",
    "        '''SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False)\n",
    "        print(clf.predict([[-0.8, -1]]))'''\n",
    "        features = extract_features(test[es_set][\"essay\"], feature_functions)\n",
    "        predicted_scores = clf.predict(features)\n",
    "        for pred_id, pred_score in zip(test[es_set][\"prediction_id\"], \n",
    "                                       predicted_scores):\n",
    "            predictions[pred_id] = round(pred_score)\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40]\n"
     ]
    }
   ],
   "source": [
    "print predicted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission to length_benchmark_svc.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = \"length_benchmark_svc.csv\"\n",
    "print(\"Writing submission to %s\" % output_file)\n",
    "f = open(output_file, \"w\")\n",
    "f.write(\"prediction_id,predicted_score\\n\")\n",
    "for key in sorted(predictions.keys()):\n",
    "   f.write(\"%d,%d\\n\" % (key,predictions[key]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "creating word vectors...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-1956db73ae55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKaggleWord2VecUtility\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreviewto_wordlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tarzilams/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1992\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tarzilams/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1999\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tarzilams/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1346\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tarzilams/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3225\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3226\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tarzilams/anaconda2/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1876\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4027)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3891)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12408)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review'"
     ]
    }
   ],
   "source": [
    "#Word_to_vec implementation \n",
    "\n",
    "#loding all required libraries\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#loading test and train data\n",
    "print \"loading data...\"\n",
    "if __name__=='__main__':\n",
    "    train=pd.read_csv('Desktop/hackathon/ASAP-AES/Data/training_set_rel3.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "    test=pd.read_csv('Desktop/hackathon/ASAP-AES/Data/valid_set.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "\n",
    "#word2vec\n",
    "print \"creating word vectors...\"\n",
    "\n",
    "clean_train_reviews=[]\n",
    "for i in xrange(len(train[\"review\"])):\n",
    "    clean_train_reviews.append(\" \".join(KaggleWord2VecUtility.reviewto_wordlist(train[\"review\"][i],True)))\n",
    "\n",
    "#create Bag of Words\n",
    "print \"creating a vector...\"\n",
    "vector=TfidfVectorizer(analyzer=\"word\",max_features=50000,sublinear_tf=True,stop_words = 'english',ngram_range=(1, 2), use_idf=1,smooth_idf=1,strip_accents='unicode',min_df=3)\n",
    "\n",
    "#tokenizing the vectors\n",
    "print \"tokenizing the vector...\" \n",
    "vector=vector.fit(clean_train_reviews)\n",
    "train_data=vector.transform(clean_train_reviews)\n",
    "\n",
    "\n",
    "y=train[\"sentiment\"]\n",
    "\n",
    "#splitting train data for testing purposes\n",
    "print \"splitting training data for testing purposes...\"\n",
    "X_train,X_test,y_train,y_test=train_test_split(train_data,y,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "showdown=False\n",
    "op=True\n",
    "\n",
    "#showdown(removed Gaussian as performed poorly)\n",
    "if showdown:\n",
    "    print \"Classifier Tasks\"\n",
    "    classifiers=[\n",
    "                RandomForestClassifier(n_estimators=150),\n",
    "                MultinomialNB(alpha=0.0001),\n",
    "                SGDClassifier(loss='modified_huber',warm_start=\"True\"),\n",
    "                LogisticRegression(penalty=\"l2\",C=1)\n",
    "                ]\n",
    "    count=0\n",
    "    for clf in classifiers:\n",
    "        count+=1\n",
    "        print \"training \",count\n",
    "        clf.fit(X_train,y_train)\n",
    "        print \"testing \",count\n",
    "        y_pred=clf.predict(X_test)\n",
    "        print \"result \",count,\":\",accuracy_score(y_test,y_pred)\n",
    "if op:\n",
    "    print \"training classifier\"\n",
    "    clf=SVC() #performing better than others\n",
    "    clf.fit(train_data,y)\n",
    "\n",
    "    print \"training complete\"\n",
    "\n",
    "    clean_test_reviews=[]\n",
    "    print \"creating test data\"\n",
    "    for i in xrange(len(test[\"review\"])):\n",
    "        clean_test_reviews.append(\" \".join(KaggleWord2VecUtility.reviewto_wordlist(test[\"review\"][i],True)))\n",
    "    test_data=vector.transform(clean_test_reviews)\n",
    "\n",
    "    print \"testing...\"\n",
    "    y_pred=clf.predict_proba(test_data)[:,1]\n",
    "    print \"testing complete\"\n",
    "    print \"preparing submission file\"\n",
    "    submission=pd.DataFrame(data={\"id\":test['id'],\"sentiment\":y_pred})\n",
    "    submission.to_csv('asap_word_to_vec.csv',quoting=3,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
